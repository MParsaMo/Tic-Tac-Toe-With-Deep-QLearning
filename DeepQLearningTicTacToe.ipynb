{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MParsaMo/Tic-Tac-Toe-With-Deep-QLearning/blob/main/DeepQLearningTicTacToe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vY-iunRqlHsd"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import numpy as np\n",
        "\n",
        "BLANK = ' '\n",
        "AI_PLAYER = 'X'\n",
        "HUMAN_PLAYER = 'O'\n",
        "TRAINING_EPOCHS = 20000\n",
        "TRAINING_EPSILON = 0.4\n",
        "REWARD_WIN = 10\n",
        "REWARD_LOSE = -10\n",
        "REWARD_TIE = 0\n",
        "\n",
        "\n",
        "class Player:\n",
        "\n",
        "    @staticmethod\n",
        "    def show_board(board):\n",
        "        print('|'.join(board[0:3]))\n",
        "        print('|'.join(board[3:6]))\n",
        "        print('|'.join(board[6:9]))\n",
        "\n",
        "\n",
        "class HumanPlayer(Player):\n",
        "\n",
        "    def reward(self, value, board):\n",
        "        pass\n",
        "\n",
        "    def make_move(self, board):\n",
        "\n",
        "        while True:\n",
        "            try:\n",
        "                self.show_board(board)\n",
        "                move = input('Your next move (cell index 1-9):')\n",
        "                move = int(move)\n",
        "                if not (move - 1 in range(9)):\n",
        "                    raise ValueError\n",
        "            except ValueError:\n",
        "                print('Invalid move; try again:\\n')\n",
        "            else:\n",
        "                return move - 1\n",
        "\n",
        "\n",
        "class AIPlayer(Player):\n",
        "\n",
        "    def __init__(self, epsilon=0.4, alpha=0.3, gamma=0.9):\n",
        "        # this is the epsilon parameter of the model: the probability of exploration\n",
        "        self.EPSILON = epsilon\n",
        "        # learning rate\n",
        "        self.ALPHA = alpha\n",
        "        # discount parameter for future reward (rewards now are better than rewards in the future)\n",
        "        self.GAMMA = gamma\n",
        "\n",
        "        # the Q(s,a) function is a neural network (this is how we eliminate the huge data structure)\n",
        "        # single hidden layer with 32 neurons - output is 1 which is the optimal move (so the index)\n",
        "        # input neurons: 36\n",
        "        self.q = Sequential()\n",
        "        self.q.add(Dense(32, input_dim=36, activation='relu'))\n",
        "        self.q.add(Dense(1, activation='relu'))\n",
        "        self.q.compile(optimizer='adam', loss='mean_squared_error')\n",
        "        # previous move during the game\n",
        "        self.move = None\n",
        "        # board in the previous iteration\n",
        "        self.board = (' ',) * 9\n",
        "\n",
        "    def available_moves(self, board):\n",
        "        return [i for i in range(9) if board[i] == ' ']\n",
        "\n",
        "    def encode_input(self, board, action):\n",
        "        # we represent the (s,a) pair with a one-dimensional array (one-hot reresentation)\n",
        "        vector_representation = []\n",
        "\n",
        "        # one-hot encoding for the 3 states\n",
        "        # [1,0,0] - it means the given cell has X ticker\n",
        "        # [0,1,0] - it means the given cell has O ticker\n",
        "        # [0,0,1] - it means the given cell has ' ' ticker so empty\n",
        "        # every single cell on the board (9 cells) has 3 values because of this representation\n",
        "        # so there are 9x3=27 values\n",
        "        for cell in board:\n",
        "            for ticker in ['X', 'O', ' ']:\n",
        "                if cell == ticker:\n",
        "                    vector_representation.append(1)\n",
        "                else:\n",
        "                    vector_representation.append(0)\n",
        "\n",
        "        # one-hot encoding of the action - array with size 9\n",
        "        # [1,0,0,0,0,0,0,0,0] - it means putting X to the first cell\n",
        "        # [0,1,0,0,0,0,0,0,0] - it means putting X to the second cell\n",
        "        for move in range(9):\n",
        "            if action == move:\n",
        "                vector_representation.append(1)\n",
        "            else:\n",
        "                vector_representation.append(0)\n",
        "\n",
        "        # all together there are 27+9=36 values (the input layer has 36 neurons)\n",
        "        return np.array([vector_representation])\n",
        "\n",
        "    # make a random move with epsilon probability (exploration) or pick the action with the\n",
        "    # highest Q value (exploitation)\n",
        "    def make_move(self, board):\n",
        "\n",
        "        self.board = tuple(board)\n",
        "        actions = self.available_moves(board)\n",
        "\n",
        "        # action with epsilon probability\n",
        "        if random.random() < self.EPSILON:\n",
        "            # this is in index (0-8 board cell related index)\n",
        "            self.move = random.choice(actions)\n",
        "            return self.move\n",
        "\n",
        "        # take the action with highest Q value\n",
        "        q_values = [self.get_q(self.board, a) for a in actions]\n",
        "        max_q_value = max(q_values)\n",
        "\n",
        "        # if multiple best actions, choose one at random\n",
        "        if q_values.count(max_q_value) > 1:\n",
        "            best_actions = [i for i in range(len(actions)) if q_values[i] == max_q_value]\n",
        "            best_move = actions[random.choice(best_actions)]\n",
        "        # there is just a single best move (best action)\n",
        "        else:\n",
        "            best_move = actions[q_values.index(max_q_value)]\n",
        "\n",
        "        self.move = best_move\n",
        "        return self.move\n",
        "\n",
        "    # Q(s,a) - we have the input (s,a) representation and the neural network will make\n",
        "    # a prediction (returns the Q value - this value will be learned during training)\n",
        "    def get_q(self, state, action):\n",
        "        return self.q.predict([self.encode_input(state, action)], batch_size=1)\n",
        "\n",
        "    # let's evaluate a given state: so update the Q(s,a) table regarding s state and a action\n",
        "    def reward(self, reward, board):\n",
        "        if self.move:\n",
        "            prev_q = self.get_q(self.board, self.move)\n",
        "            max_q_new = max([self.get_q(tuple(board), a) for a in self.available_moves(self.board)])\n",
        "            # train the neural network with the new (s,a) and Q value\n",
        "            self.q.fit(self.encode_input(self.board, self.move),\n",
        "                       prev_q + self.ALPHA * ((reward + self.GAMMA * max_q_new) - prev_q),\n",
        "                       epochs=3, verbose=0)\n",
        "\n",
        "        self.move = None\n",
        "        self.board = None\n",
        "\n",
        "\n",
        "class TicTacToe:\n",
        "\n",
        "    def __init__(self, player1, player2):\n",
        "        self.player1 = player1\n",
        "        self.player2 = player2\n",
        "        self.first_player_turn = random.choice([True, False])\n",
        "        self.board = [' '] * 9\n",
        "\n",
        "    def play(self):\n",
        "\n",
        "        # this is the \"game loop\"\n",
        "        while True:\n",
        "            if self.first_player_turn:\n",
        "                player = self.player1\n",
        "                other_player = self.player2\n",
        "                player_tickers = (AI_PLAYER, HUMAN_PLAYER)\n",
        "            else:\n",
        "                player = self.player2\n",
        "                other_player = self.player1\n",
        "                player_tickers = (HUMAN_PLAYER, AI_PLAYER)\n",
        "\n",
        "            # check the state of the game (win, lose or draw)\n",
        "            game_over, winner = self.is_game_over(player_tickers)\n",
        "\n",
        "            # game is over: handle the rewards\n",
        "            if game_over:\n",
        "                if winner == player_tickers[0]:\n",
        "                    player.show_board(self.board[:])\n",
        "                    print('\\n %s won!' % player.__class__.__name__)\n",
        "                    player.reward(REWARD_WIN, self.board[:])\n",
        "                    other_player.reward(REWARD_LOSE, self.board[:])\n",
        "                if winner == player_tickers[1]:\n",
        "                    player.show_board(self.board[:])\n",
        "                    print('\\n %s won!' % other_player.__class__.__name__)\n",
        "                    other_player.reward(REWARD_WIN, self.board[:])\n",
        "                    player.reward(REWARD_LOSE, self.board[:])\n",
        "                else:\n",
        "                    player.show_board(self.board[:])\n",
        "                    print('Tie!')\n",
        "                    player.reward(REWARD_TIE, self.board[:])\n",
        "                    other_player.reward(REWARD_TIE, self.board[:])\n",
        "                break\n",
        "\n",
        "            # next player's turn in the next iteration\n",
        "            self.first_player_turn = not self.first_player_turn\n",
        "\n",
        "            # actual player's best move (based on Q(s,a) table for AI player)\n",
        "            move = player.make_move(self.board)\n",
        "            self.board[move] = player_tickers[0]\n",
        "\n",
        "    def is_game_over(self, player_tickers):\n",
        "\n",
        "        # consider both players (X and O players - these are the tickers)\n",
        "        for player_ticker in player_tickers:\n",
        "\n",
        "            # check horizontal dimension (so the rows)\n",
        "            for i in range(3):\n",
        "                if self.board[3 * i + 0] == player_ticker and\\\n",
        "                        self.board[3 * i + 1] == player_ticker and\\\n",
        "                        self.board[3 * i + 2] == player_ticker:\n",
        "                    return True, player_ticker\n",
        "\n",
        "            # check vertical dimension (so the columns)\n",
        "            for j in range(3):\n",
        "                if self.board[j + 0] == player_ticker and\\\n",
        "                        self.board[j + 3] == player_ticker and\\\n",
        "                        self.board[j + 6] == player_ticker:\n",
        "                    return True, player_ticker\n",
        "\n",
        "            # check diagonal dimensions (top left to bottom right + top right to bottom left)\n",
        "            if self.board[0] == player_ticker and self.board[4] == player_ticker and self.board[8] == player_ticker:\n",
        "                return True, player_ticker\n",
        "\n",
        "            if self.board[2] == player_ticker and self.board[4] == player_ticker and self.board[6] == player_ticker:\n",
        "                return True, player_ticker\n",
        "\n",
        "        # finally we can deal with the 'draw' cases\n",
        "        if self.board.count(' ') == 0:\n",
        "            return True, None\n",
        "        else:\n",
        "            return False, None\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    ai_player_1 = AIPlayer()\n",
        "    ai_player_2 = AIPlayer()\n",
        "\n",
        "    print('Training')\n",
        "    ai_player_1.EPSILON = TRAINING_EPSILON\n",
        "    ai_player_1.EPSILON = TRAINING_EPSILON\n",
        "\n",
        "    for i in range(TRAINING_EPOCHS):\n",
        "        print(\"Training iteration %s\" % i)\n",
        "        game = TicTacToe(ai_player_1, ai_player_2)\n",
        "        game.play()\n",
        "\n",
        "    print('\\nTraining is Done')\n",
        "\n",
        "    # epsilon=0 means no exploration - it will use the Q(s,a) function to make the moves\n",
        "    ai_player_1.EPSILON = 0\n",
        "    human_player = HumanPlayer()\n",
        "    game = TicTacToe(ai_player_1, human_player)\n",
        "    game.play()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4221476"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyOUmmOPzm/2RuSl+/LrrOd3",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}